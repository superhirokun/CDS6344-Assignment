{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f036f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from datetime import datetime\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a51617",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f48b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('steam_game_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d8ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a3370",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hours_played\"] = pd.to_numeric(df[\"hours_played\"], errors=\"coerce\")\n",
    "df[\"helpful\"] = pd.to_numeric(df[\"helpful\"], errors=\"coerce\")\n",
    "df[\"funny\"] = pd.to_numeric(df[\"funny\"], errors=\"coerce\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a4f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f1cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='recommendation', data=df)\n",
    "plt.title('Distribution of Recommendation')\n",
    "plt.xlabel('Recommendation')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c797b29",
   "metadata": {},
   "source": [
    "A subset of 100k rows of data is extracted from the dataset with the same ratio of recommended and not recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff06920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended = df[df['recommendation'] == 'Recommended']\n",
    "not_recommended = df[df['recommendation'] == 'Not Recommended']\n",
    "\n",
    "n_not_recommended = 20000\n",
    "n_recommended = int(n_not_recommended * 8 / 2)\n",
    "\n",
    "sampled_recommended = recommended.sample(n=n_recommended, random_state=42)\n",
    "sampled_not_recommended = not_recommended.sample(n=n_not_recommended, random_state=42)\n",
    "\n",
    "df_sample = pd.concat([sampled_recommended, sampled_not_recommended]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(df_sample['recommendation'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a8667",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended = df[df['recommendation'] == 'Recommended']\n",
    "not_recommended = df[df['recommendation'] == 'Not Recommended']\n",
    "\n",
    "n_not_recommended = 20000\n",
    "n_recommended = int(n_not_recommended * 8 / 2)\n",
    "\n",
    "sampled_recommended = recommended.sample(n=n_recommended, random_state=42)\n",
    "sampled_not_recommended = not_recommended.sample(n=n_not_recommended, random_state=42)\n",
    "\n",
    "df_sample_opinion_absa = pd.concat([sampled_recommended, sampled_not_recommended]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(df_sample_opinion_absa['recommendation'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938bdd8e",
   "metadata": {},
   "source": [
    "# Data Preproccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6027b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6168e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ae4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # 1. Text Normalization (Lowercasing)\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 3. Stopword Removal and Lemmatization\n",
    "    processed_tokens = []\n",
    "    for word in tokens:\n",
    "        if word.isalpha() and word not in stop_words:\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "            processed_tokens.append(lemma)\n",
    "\n",
    "    # Join tokens back into a string\n",
    "    return \" \".join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b3c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['processed_review'] = df_sample['review'].apply(preprocess_text)\n",
    "df_sample_opinion_absa['processed_review'] = df_sample_opinion_absa['review'].apply(preprocess_text)\n",
    "\n",
    "print(\"Processed Reviews:\")\n",
    "print(df_sample['processed_review'].head())\n",
    "print(\"Processed Reviews for Opinion ABSA:\")\n",
    "print(df_sample_opinion_absa['processed_review'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6098eed6",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266983b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_values = df_sample['recommendation'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=count_values.index, y=count_values.values, palette='magma')\n",
    "plt.title('Comparison of values from the recommendation column')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Number of Recommendations')\n",
    "for i, v in enumerate(count_values.values):\n",
    "    plt.text(i, v + 0.1, str(v), ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f88c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_sample['date']\n",
    "df2 = pd.DataFrame(data)\n",
    "\n",
    "def preprocess_dates(date_str):\n",
    "    if ',' not in date_str:\n",
    "        return f\"{date_str}, 2024\"\n",
    "    return date_str\n",
    "\n",
    "df_sample['date'] = df_sample['date'].apply(preprocess_dates)\n",
    "\n",
    "df_sample['date'] = pd.to_datetime(df_sample['date'], format='mixed')\n",
    "\n",
    "df_sample['year'] = df_sample['date'].dt.year\n",
    "\n",
    "recommendation_counts_per_year = df_sample.groupby(['year', 'recommendation']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed39197",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_counts_per_year.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Number of Recommended/Not Recommended Reviews per Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Recommendation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3044031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_reviews = \" \".join(df_sample[df_sample['recommendation'].str.lower() == 'recommended']['review'].dropna())\n",
    "not_recommended_reviews = \" \".join(df_sample[df_sample['recommendation'].str.lower() == 'not recommended']['review'].dropna())\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"game\", \"games\", \"play\", \"time\", \"one\", \"get\", \"early\", \"access\", \"make\", \"even\", \"review\", \"want\", \"still\", \"will\", \"feel\", \"lot\", \"now\"]) # Adding specific game-related words to the list of stopwords\n",
    "\n",
    "if len(recommended_reviews) > 0:\n",
    "  wc_recommended = WordCloud(width=800, height=400, background_color=\"white\", stopwords=stopwords).generate(recommended_reviews)\n",
    "  plt.figure(figsize=(8, 4))\n",
    "  plt.title(\"Recommended Reviews Word Cloud\")\n",
    "  plt.imshow(wc_recommended, interpolation=\"bilinear\")\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()\n",
    "else:\n",
    "    print(\"No 'recommended' reviews found to generate word cloud.\")\n",
    "\n",
    "if len(not_recommended_reviews) > 0:\n",
    "  wc_not_recommended = WordCloud(width=800, height=400, background_color=\"white\", stopwords=stopwords).generate(not_recommended_reviews)\n",
    "  plt.figure(figsize=(8, 4))\n",
    "  plt.title(\"Not Recommended Reviews Word Cloud\")\n",
    "  plt.imshow(wc_not_recommended, interpolation=\"bilinear\")\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()\n",
    "else:\n",
    "  print(\"No 'not recommended' reviews found to generate word cloud.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of hours played for each recommendation type\n",
    "sns.boxplot(data=df_sample, x='recommendation', y='hours_played')\n",
    "plt.title(\"Hours Played vs Recommendation\")\n",
    "plt.xlabel(\"Recommendation\")\n",
    "plt.ylabel(\"Hours Played\")\n",
    "plt.show()\n",
    "\n",
    "# Filter extreme outliers for better visualisation\n",
    "df_filtered = df_sample[df_sample['hours_played'] < df_sample['hours_played'].quantile(0.99)]\n",
    "sns.histplot(data=df_filtered, x='hours_played', hue='recommendation', kde=True, bins=50)\n",
    "plt.title(\"Hours Played Distribution\")\n",
    "plt.xlabel(\"Hours Played\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e8d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_sample[['hours_played', 'helpful', 'funny']].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea32ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.groupby('recommendation')[['helpful', 'funny']].mean().plot(kind='bar')\n",
    "plt.title(\"Average Helpful and Funny Scores by Recommendation\")\n",
    "plt.ylabel(\"Average Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0939a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['date'].value_counts().sort_index().plot(kind='line', figsize=(12, 6))\n",
    "plt.title(\"Number of Reviews Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.show()\n",
    "\n",
    "# Extract month or year for additional trends\n",
    "df_sample['year'] = df_sample['date'].dt.year\n",
    "df_sample['month'] = df_sample['date'].dt.month\n",
    "\n",
    "# Monthly trends\n",
    "sns.countplot(data=df_sample, x='month', hue='year', palette='viridis')\n",
    "plt.title(\"Monthly Review Trends\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd71a6b4",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1802242",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_sample['processed_review'])\n",
    "\n",
    "print(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\")\n",
    "print(f\"This matrix has {tfidf_matrix.shape[0]} documents (reviews) and {tfidf_matrix.shape[1]} unique features (words).\")\n",
    "print(\"TF-IDF features successfully generated. This sparse matrix is ready for Traditional ML Models.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb365cc1",
   "metadata": {},
   "source": [
    "Make sure the `glove.6B.100d.txt` is in the same folder when running this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1135d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding_dim = 100\n",
    "glove_path = 'glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4495b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path):\n",
    "    embeddings_index = {}\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"ERROR: GloVe file '{path}' not found.\")\n",
    "        return {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5677e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = load_glove_embeddings(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d59bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_avg_word_embedding(text_tokens_string, embeddings_index, embedding_dim):\n",
    "    tokens = text_tokens_string.split()\n",
    "    vectors = []\n",
    "    for word in tokens:\n",
    "        if word in embeddings_index:\n",
    "            vectors.append(embeddings_index[word])\n",
    "    \n",
    "    if not vectors:\n",
    "        return np.zeros(embedding_dim)\n",
    "    \n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3414466",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['glove_features'] = df_sample['review'].apply(\n",
    "    lambda x: create_avg_word_embedding(x, embedding_index, glove_embedding_dim).tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f433c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_features_matrix = np.array(df_sample['glove_features'].tolist())\n",
    "\n",
    "print(f\"Shape of GloVe features matrix: {glove_features_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba80708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586680cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561d284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model.to(device)\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c652d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ReviewsDataset(df_sample['review'].tolist(), bert_tokenizer, 64)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eba70c",
   "metadata": {},
   "source": [
    "The contextual embeddings (BERT embeddings) process, the embeddings is saved to a `.npy` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd6195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_embeddings = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in tqdm(dataloader, desc=\"Extracting Embeddings\"):\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#         outputs =  bert_model(**batch)\n",
    "#         last_hidden_state = outputs.last_hidden_state\n",
    "        \n",
    "#         cls_embeddings = last_hidden_state[:, 0, :] \n",
    "#         all_embeddings.append(cls_embeddings.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb235892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_embeddings = torch.cat(all_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e470c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np_embeddings = all_embeddings.numpy()\n",
    "# np.save(\"steam_review_embeddings.npy\", np_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ffd7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment_classification = df_sample.copy()\n",
    "\n",
    "df_sentiment_classification['recommendation'] = df_sentiment_classification['recommendation'].apply(lambda x: 1 if x == 'Recommended' else 0)\n",
    "\n",
    "df_sentiment_classification.drop(columns=['review', 'date', 'game_name', 'username', 'processed_review', 'glove_features'], inplace=True)\n",
    "\n",
    "df_sentiment_classification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af7cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_embeddings = np.load(\"steam_review_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d617589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_feature_df = pd.DataFrame(np_embeddings, columns=[f'bert_dim_{i}' for i in range(np_embeddings.shape[1])])\n",
    "df_sentiment_classification = pd.concat([df_sentiment_classification, bert_feature_df], axis=1)\n",
    "print(f\"\\nBERT embeddings added to the DataFrame. New shape: {df_sentiment_classification.shape}\")\n",
    "print(df_sentiment_classification.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d93a31b",
   "metadata": {},
   "source": [
    "# Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665f405",
   "metadata": {},
   "source": [
    "### Traditional Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aedaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 200 ,random_state=50,  n_jobs=-1)\n",
    "xgb = XGBClassifier(n_estimators = 200, eval_metric='logloss', random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f74fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_sentiment_classification.drop('recommendation', axis=1) \n",
    "y = df_sentiment_classification['recommendation']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7842b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b8e922",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning (Failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d15998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_model = RandomForestClassifier(random_state=50)\n",
    "# xgb_model = XGBClassifier(eval_metric='logloss', random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cebe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import randint\n",
    "\n",
    "# param_dist_rf = {\n",
    "#     'n_estimators': randint(100, 500), # Number of trees in the forest\n",
    "#     'max_features': ['sqrt', 'log2', 0.8, 1.0], # Number of features to consider at each split\n",
    "#     'max_depth': randint(10, 100), # Maximum depth of the tree\n",
    "#     'min_samples_split': randint(2, 20), # Minimum number of samples required to split an internal node\n",
    "#     'min_samples_leaf': randint(1, 20), # Minimum number of samples required to be at a leaf node\n",
    "#     'bootstrap': [True, False], # Whether bootstrap samples are used when building trees\n",
    "#     'criterion': ['gini', 'entropy'] # Function to measure the quality of a split\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765340e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# random_search_rf = RandomizedSearchCV(\n",
    "#     estimator=rf_model,\n",
    "#     param_distributions=param_dist_rf,\n",
    "#     n_iter=50, # Number of iterations for random search\n",
    "#     cv=5,      \n",
    "#     scoring='f1', \n",
    "#     random_state=42,\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaaa11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import uniform\n",
    "\n",
    "# param_dist_xgb = {\n",
    "#     'n_estimators': randint(100, 1000), # Number of boosting rounds (trees)\n",
    "#     'learning_rate': uniform(0.01, 0.3), # Step size shrinkage\n",
    "#     'max_depth': randint(3, 10), # Maximum depth of a tree\n",
    "#     'subsample': uniform(0.6, 0.4), # Subsample ratio of the training instance\n",
    "#     'colsample_bytree': uniform(0.6, 0.4), # Subsample ratio of columns when constructing each tree\n",
    "#     'gamma': uniform(0, 0.5), # Minimum loss reduction required to make a further partition\n",
    "#     'lambda': uniform(0.5, 1.5), # L2 regularization term on weights\n",
    "#     'alpha': uniform(0, 0.5) # L1 regularization term on weights\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f12d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_search_xgb = RandomizedSearchCV(\n",
    "#     estimator=xgb_model,\n",
    "#     param_distributions=param_dist_xgb,\n",
    "#     n_iter=20, # Number of iterations for hyperparameter search\n",
    "#     cv=5,      \n",
    "#     scoring='accuracy',\n",
    "#     random_state=42,\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76caac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# print(\"\\nRandom Forest Random Search Results:\")\n",
    "# print(f\"Best parameters found: {random_search_rf.best_params_}\")\n",
    "# print(f\"Best cross-validation accuracy: {random_search_rf.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2077ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# print(\"\\n--- Best XGBoost Model Evaluation on Test Set ---\")\n",
    "# print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f007b07",
   "metadata": {},
   "source": [
    "### Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn = df_sample.copy()\n",
    "df_nn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc259b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn['recommendation'] = df_nn['recommendation'].apply(lambda x: 1 if x == 'Recommended' else 0)\n",
    "df_nn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0094ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_nn['recommendation']\n",
    "\n",
    "np.save(\"labels.npy\", labels)\n",
    "\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f308e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"steam_review_embeddings.npy\")\n",
    "y = np.load(\"labels.npy\")\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a08445",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNN(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=128, dropout=0.3):\n",
    "        super(SentimentNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return torch.sigmoid(self.fc2(x)).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb61624",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 100\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for xb, yb in val_loader:\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            val_loss += loss.item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_losses[-1]:.6f}, Val Loss = {val_losses[-1]:.6f}\")\n",
    "    torch.save(model.state_dict(), f\"nn8020/sentiment_nn_8020_epoch_{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44e4f33",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a0de1",
   "metadata": {},
   "source": [
    "### Traditional Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4126354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Accuracy\n",
    "print(f\"Accuracy: {metrics.accuracy_score(y_test, y_pred_rf)}\")\n",
    "\n",
    "# Random Forest Precision\n",
    "print(f\"Precision: {metrics.precision_score(y_test, y_pred_rf)}\")\n",
    "\n",
    "# Random Forest Recall\n",
    "print(f\"Recall: {metrics.recall_score(y_test, y_pred_rf)}\")\n",
    "\n",
    "# Random Forest F1 Score\n",
    "print(f\"F1 Score: {metrics.f1_score(y_test, y_pred_rf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab5daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Accuracy\n",
    "print(f\"Accuracy: {metrics.accuracy_score(y_test, y_pred_xgb)}\")\n",
    "\n",
    "# XGBoost Precision\n",
    "print(f\"Precision: {metrics.precision_score(y_test, y_pred_xgb)}\")\n",
    "\n",
    "# XGBoost Recall\n",
    "print(f\"Recall: {metrics.recall_score(y_test, y_pred_xgb)}\")\n",
    "\n",
    "# XGBoost F1 Score\n",
    "print(f\"F1 Score: {metrics.f1_score(y_test, y_pred_xgb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a632f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Random Forest\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=[\"Not Recommended\", \"Recommended\"])\n",
    "disp_rf.plot(cmap=\"Blues\")\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e1acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for XGBoost\n",
    "cm_xg = confusion_matrix(y_test, y_pred_xgb)\n",
    "disp_xg = ConfusionMatrixDisplay(confusion_matrix=cm_xg, display_labels=[\"Not Recommended\", \"Recommended\"])\n",
    "disp_xg.plot(cmap=\"Blues\")\n",
    "plt.title(\"XGBoost Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3025cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define k-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa5e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest cross-validation for accuracy\n",
    "rf_accuracy = cross_val_score(rf, X, y, cv=kfold, scoring='accuracy', n_jobs=-1)\n",
    "print(f\"Random Forest 3-Fold CV Accuracy: {rf_accuracy}\")\n",
    "print(f\"Random Forest Mean Accuracy: {rf_accuracy.mean():.4f} (+/- {rf_accuracy.std():.4f})\")\n",
    "\n",
    "# XGBoost cross-validation for accuracy\n",
    "xgb_accuracy = cross_val_score(xgb, X, y, cv=kfold, scoring='accuracy', n_jobs=-1)\n",
    "print(f\"XGBoost 3-Fold CV Accuracy: {xgb_accuracy}\")\n",
    "print(f\"XGBoost Mean Accuracy: {xgb_accuracy.mean():.4f} (+/- {xgb_accuracy.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest cross-validation for precision\n",
    "rf_precision = cross_val_score(rf, X, y, cv=kfold, scoring='precision', n_jobs=-1)\n",
    "print(f\"Random Forest 3-Fold CV Precision: {rf_precision}\")\n",
    "print(f\"Random Forest Mean Precision: {rf_precision.mean():.4f} (+/- {rf_precision.std():.4f})\")\n",
    "\n",
    "# XGBoost cross-validation for precision\n",
    "xgb_precision = cross_val_score(xgb, X, y, cv=kfold, scoring='precision', n_jobs=-1)\n",
    "print(f\"XGBoost 3-Fold CV Precision: {xgb_precision}\")\n",
    "print(f\"XGBoost Mean Precision: {xgb_precision.mean():.4f} (+/- {xgb_precision.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ddde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest cross-validation for recall\n",
    "rf_recall = cross_val_score(rf, X, y, cv=kfold, scoring='recall', n_jobs=-1)\n",
    "print(f\"Random Forest 3-Fold CV Recall: {rf_recall}\")\n",
    "print(f\"Random Forest Mean Recall: {rf_recall.mean():.4f} (+/- {rf_recall.std():.4f})\")\n",
    "\n",
    "# XGBoost cross-validation for recall\n",
    "xgb_recall = cross_val_score(xgb, X, y, cv=kfold, scoring='recall', n_jobs=-1)\n",
    "print(f\"XGBoost 3-Fold CV Recall: {xgb_recall}\")\n",
    "print(f\"XGBoost Mean Recall: {xgb_recall.mean():.4f} (+/- {xgb_recall.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest cross-validation for F1 score\n",
    "rf_scores = cross_val_score(rf, X, y, cv=kfold, scoring='f1', n_jobs=-1)\n",
    "print(f\"Random Forest 3-Fold CV F1 Scores: {rf_scores}\")\n",
    "print(f\"Random Forest Mean F1: {rf_scores.mean():.4f} (+/- {rf_scores.std():.4f})\")\n",
    "\n",
    "# XGBoost cross-validation for F1 score\n",
    "xgb_scores = cross_val_score(xgb, X, y, cv=kfold, scoring='f1', n_jobs=-1)\n",
    "print(f\"XGBoost 3-Fold CV F1 Scores: {xgb_scores}\")\n",
    "print(f\"XGBoost Mean F1: {xgb_scores.mean():.4f} (+/- {xgb_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a254e",
   "metadata": {},
   "source": [
    "### Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc44e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel(\"Epoch+1\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a35547",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(1, 100):\n",
    "    model.load_state_dict(torch.load(f'nn8020/sentiment_nn_8020_epoch_{epoch}.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    # Validation accuracy\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            preds = model(xb)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(yb.cpu().numpy())\n",
    "    val_binary = [1 if p >= 0.5 else 0 for p in val_preds]\n",
    "    val_accuracies.append(accuracy_score(val_labels, val_binary))\n",
    "\n",
    "    # Training accuracy\n",
    "    train_preds, train_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in train_loader:\n",
    "            preds = model(xb)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(yb.cpu().numpy())\n",
    "    train_binary = [1 if p >= 0.5 else 0 for p in train_preds]\n",
    "    train_accuracies.append(accuracy_score(train_labels, train_binary))\n",
    "\n",
    "# Plot graph\n",
    "plt.plot(range(1, 100), train_accuracies, label=\"Train Accuracy\", color=\"blue\")\n",
    "plt.plot(range(1, 100), val_accuracies, label=\"Validation Accuracy\", color=\"orange\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Train vs Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1411316",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentNN()\n",
    "model.load_state_dict(torch.load(\"nn8020/sentiment_nn_8020_epoch_60.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Predict on validation set\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        preds = model(xb)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "# Binarize predictions\n",
    "binary_preds = [1 if p >= 0.5 else 0 for p in all_preds]\n",
    "\n",
    "# Metrics\n",
    "print(f\"Accuracy: {metrics.accuracy_score(all_labels, binary_preds)}\")\n",
    "print(f\"Precision: {metrics.precision_score(all_labels, binary_preds)}\")\n",
    "print(f\"Recall: {metrics.recall_score(all_labels, binary_preds)}\")\n",
    "print(f\"F1 Score: {metrics.f1_score(all_labels, binary_preds)}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, binary_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Not Recommended\", \"Recommended\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Neural Network Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5da536",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc453f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment_analysis = df_sample.copy()\n",
    "df_sentiment_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0decf91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment_analysis['recommendation'] = df_sentiment_analysis['recommendation'].apply(lambda x: 1 if x == 'Recommended' else 0)\n",
    "df_sentiment_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8058519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_sentiment_analysis['processed_review'], df_sentiment_analysis['recommendation'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef70fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539484f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "model_xgb = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "model_xgb.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe2d80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "preds_rf = model_rf.predict(X_test_vec)\n",
    "preds_xgb = model_xgb.predict(X_test_vec)\n",
    "print(\"Random Forest:\\n\", classification_report(y_test, preds_rf))\n",
    "print(\"XGBoost:\\n\", classification_report(y_test, preds_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66362293",
   "metadata": {},
   "source": [
    "### Deep Learning - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a481c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec7bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment_analysis = df_sample.copy()\n",
    "df_sentiment_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e910ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch dataset for easy batching\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77747591",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ReviewsDataset(df_sentiment_analysis['processed_review'].tolist(), tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b44da",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Processing Batches\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        all_predictions.extend(preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1474b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# True labels\n",
    "df_sentiment_analysis['recommendation'] = df_sentiment_analysis['recommendation'].apply(lambda x: 1 if x == 'Recommended' else 0)\n",
    "true_labels = df_sentiment_analysis['recommendation'].values\n",
    "\n",
    "# Predicted labels\n",
    "predicted_labels = all_predictions\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.6f}\")\n",
    "print(f\"Precision: {precision:.6f}\")\n",
    "print(f\"Recall:    {recall:.6f}\")\n",
    "print(f\"F1 Score:  {f1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bac7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment_analysis['predicted_sentiment'] = all_predictions\n",
    "df_sentiment_analysis.to_csv(\"steam_reviews_with_sentiment.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f835d2",
   "metadata": {},
   "source": [
    "### Opinion Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b18127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02283ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_opinion_absa[['review', 'processed_review']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393123ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_opinions(text):\n",
    "    doc = nlp(text)\n",
    "    opinions = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"amod\" and token.head.pos_ == \"NOUN\":\n",
    "            opinions.append((token.text, token.head.text))  \n",
    "    return opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "def get_sentiment_score(word):\n",
    "    try:\n",
    "        synsets = list(swn.senti_synsets(word))\n",
    "        if synsets:\n",
    "            s = synsets[0]\n",
    "            return s.pos_score() - s.neg_score()\n",
    "        else:\n",
    "            return 0.0\n",
    "    except:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1292201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    tokenizer=\"distilbert-base-uncased-finetuned-sst-2-english\", \n",
    "    truncation=True,\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "def safe_sentiment(review):\n",
    "    if len(review.split()) > 500:\n",
    "        review = \" \".join(review.split()[:500])\n",
    "    result = sentiment_pipeline(review)[0]\n",
    "    return result['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb31adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "df_sample_opinion_absa['explicit_opinions'] = df_sample_opinion_absa['processed_review'].apply(extract_opinions)\n",
    "df_sample_opinion_absa['explicit_sentiment_score'] = df_sample_opinion_absa['explicit_opinions'].apply(\n",
    "    lambda pairs: sum([get_sentiment_score(op[0]) for op in pairs]) / len(pairs) if pairs else 0\n",
    ")\n",
    "df_sample_opinion_absa['implicit_sentiment'] = df_sample_opinion_absa['review'].apply(safe_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9fc001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sentiments(row):\n",
    "    if row['explicit_sentiment_score'] > 0.1 and row['implicit_sentiment'] == \"NEGATIVE\":\n",
    "        return 'Conflict: Explicit Positive, Implicit Negative'\n",
    "    elif row['explicit_sentiment_score'] < -0.1 and row['implicit_sentiment'] == \"POSITIVE\":\n",
    "        return 'Conflict: Explicit Negative, Implicit Positive'\n",
    "    else:\n",
    "        return 'Aligned'\n",
    "\n",
    "df_sample_opinion_absa['sentiment_alignment'] = df_sample_opinion_absa.apply(compare_sentiments, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967c6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_opinion_absa[['explicit_opinions','explicit_sentiment_score','implicit_sentiment','sentiment_alignment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87af072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "all_targets = list(chain.from_iterable([ [target for _, target in pair] for pair in df_sample_opinion_absa['explicit_opinions']]))\n",
    "target_counts = Counter(all_targets)\n",
    "print(target_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fac50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_targets = target_counts.most_common(10)\n",
    "top_targets_df = pd.DataFrame(top_targets, columns=['opinion_target', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9487b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "sns.countplot(x='implicit_sentiment', data=df_sample_opinion_absa, palette='viridis', order=['POSITIVE', 'NEUTRAL', 'NEGATIVE'])\n",
    "plt.title('Distribution of Overall (Implicit) Review Sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dede4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_counts = df_sample_opinion_absa['sentiment_alignment'].value_counts()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(alignment_counts, labels=alignment_counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('coolwarm'))\n",
    "plt.title('Proportion of Sentiment Alignment Categories')\n",
    "plt.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d084e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='count', y='opinion_target', data=top_targets_df, palette='cubehelix')\n",
    "plt.title(f'Top 10 Most Frequently Mentioned Aspects')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Aspect')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ce2b34",
   "metadata": {},
   "source": [
    "### Aspect-Based Sentiment Analysis (ABSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e0f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_aspects(text):\n",
    "    doc = nlp(text)\n",
    "    return [chunk.text.lower() for chunk in doc.noun_chunks if len(chunk.text.split()) <= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64128831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score(word):\n",
    "    try:\n",
    "        synsets = list(swn.senti_synsets(word))\n",
    "        if synsets:\n",
    "            s = synsets[0]\n",
    "            return s.pos_score() - s.neg_score()\n",
    "    except:\n",
    "        return 0.0\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c12438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_aspect_sentiment(text):\n",
    "    doc = nlp(text)\n",
    "    aspect_sentiment_pairs = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"amod\" and token.head.pos_ == \"NOUN\":\n",
    "            sentiment_score = get_sentiment_score(token.text)\n",
    "            sentiment_label = \"positive\" if sentiment_score > 0.1 else \"negative\" if sentiment_score < -0.1 else \"neutral\"\n",
    "            aspect_sentiment_pairs.append((token.head.text.lower(), sentiment_label))\n",
    "    return aspect_sentiment_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d3275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"yangheng/deberta-v3-base-absa-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "absa_pipeline = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb33fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_absa(text):\n",
    "    aspects = extract_aspects(text)\n",
    "    results = []\n",
    "    for asp in aspects:\n",
    "        try:\n",
    "            formatted_text = f\"{text} [ASP] {asp} [ASP]\"\n",
    "            prediction = absa_pipeline(formatted_text)[0]\n",
    "            results.append({\n",
    "                'aspect': asp,\n",
    "                'sentiment': prediction['label'],\n",
    "                'confidence': round(prediction['score'], 3)\n",
    "            })\n",
    "        except:\n",
    "            continue\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e7c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "df_sample_opinion_absa['aspects'] = df_sample_opinion_absa['processed_review'].progress_apply(extract_aspects)\n",
    "df_sample_opinion_absa['aspect_sentiment_rule'] = df_sample_opinion_absa['processed_review'].progress_apply(extract_aspect_sentiment)\n",
    "df_sample_opinion_absa['aspect_sentiment_transformer'] = df_sample_opinion_absa['processed_review'].progress_apply(transformer_absa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_opinion_absa[['aspects','aspect_sentiment_rule','aspect_sentiment_transformer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f378f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "# Convert stringified lists to actual lists if needed\n",
    "df_sample_opinion_absa['aspect_sentiment_rule'] = df_sample_opinion_absa['aspect_sentiment_rule'].apply(literal_eval)\n",
    "df_sample_opinion_absa['aspect_sentiment_transformer'] = df_sample_opinion_absa['aspect_sentiment_transformer'].apply(literal_eval)\n",
    "\n",
    "# Extract sentiments\n",
    "def extract_sentiments(sentiment_data, is_dict=False):\n",
    "    sentiments = []\n",
    "    for entry in sentiment_data:\n",
    "        if is_dict:\n",
    "            sentiments.append(entry['sentiment'].lower())\n",
    "        else:\n",
    "            sentiments.append(entry[1].lower())\n",
    "    return sentiments\n",
    "\n",
    "# Flatten and count\n",
    "rule_sentiments = df_sample_opinion_absa['aspect_sentiment_rule'].apply(extract_sentiments)\n",
    "transformer_sentiments = df_sample_opinion_absa['aspect_sentiment_transformer'].apply(lambda x: extract_sentiments(x, is_dict=True))\n",
    "\n",
    "# Flatten\n",
    "flat_rule = [s for sublist in rule_sentiments for s in sublist]\n",
    "flat_transformer = [s for sublist in transformer_sentiments for s in sublist]\n",
    "\n",
    "# Count and make DataFrame\n",
    "rule_counts = pd.Series(flat_rule).value_counts().rename_axis('Sentiment').reset_index(name='Count')\n",
    "rule_counts['Method'] = 'Rule-based'\n",
    "\n",
    "transformer_counts = pd.Series(flat_transformer).value_counts().rename_axis('Sentiment').reset_index(name='Count')\n",
    "transformer_counts['Method'] = 'Transformer-based'\n",
    "\n",
    "combined_counts = pd.concat([rule_counts, transformer_counts])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=combined_counts, x='Sentiment', y='Count', hue='Method')\n",
    "plt.title(\"Sentiment Counts: Rule-based vs Transformer-based ABSA\")\n",
    "plt.ylabel(\"Number of Aspects\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a2c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_sentiments = []\n",
    "\n",
    "for row in df_sample_opinion_absa['aspect_sentiment_transformer']:\n",
    "    for item in row:\n",
    "        aspect_sentiments.append((item['aspect'], item['sentiment']))\n",
    "\n",
    "df_absa = pd.DataFrame(aspect_sentiments, columns=['aspect', 'sentiment'])\n",
    "top_aspects = df_absa['aspect'].value_counts().head(10).index\n",
    "filtered = df_absa[df_absa['aspect'].isin(top_aspects)]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data=filtered, x='aspect', hue='sentiment')\n",
    "plt.title('Sentiment Distribution for Top Aspects using Transformer ABSA')\n",
    "plt.xticks(rotation=40)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd710e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_sentiments = []\n",
    "\n",
    "for row in df_sample_opinion_absa['aspect_sentiment_rule']:\n",
    "    for item in row:\n",
    "        aspect_sentiments.append((item[0], item[1]))\n",
    "\n",
    "df_absa = pd.DataFrame(aspect_sentiments, columns=['aspect', 'sentiment'])\n",
    "top_aspects = df_absa['aspect'].value_counts().head(10).index\n",
    "filtered = df_absa[df_absa['aspect'].isin(top_aspects)]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data=filtered, x='aspect', hue='sentiment')\n",
    "plt.title('Sentiment Distribution for Top Aspects Using Rule-Based ABSA')\n",
    "plt.xticks(rotation=40)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib-venn\n",
    "\n",
    "from matplotlib_venn import venn2\n",
    "rule_ids = set(df_sample_opinion_absa[df_sample_opinion_absa['aspect_sentiment_rule'].apply(lambda x: len(x) > 0)].index)\n",
    "transformer_ids = set(df_sample_opinion_absa[df_sample_opinion_absa['aspect_sentiment_transformer'].apply(lambda x: len(x) > 0)].index)\n",
    "\n",
    "# Plot Venn Diagram\n",
    "plt.figure(figsize=(6,6))\n",
    "venn2([rule_ids, transformer_ids], set_labels=('Rule-based ABSA', 'Transformer-based ABSA'))\n",
    "plt.title(\"Overlap of Reviews with Aspect Sentiments\\n(Rule vs Transformer Method)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
